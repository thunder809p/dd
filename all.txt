lr

data = read.csv("./income.data.csv")
View(data)
str(data)
summary(data)
install.packages("ggpubr")
library(ggplot2)
library(dplyr)
library(broom)
library(ggpubr)


#hist(data$income)
hist(data$happiness)
plot(data$income,data$happiness,xlab = "income",ylab = "happiness",main = "income vs happiness scatter plot")

lin = lm(happiness ~ income, data = data)
summary(lin)

plot(data$income,data$happiness)
abline(lin)

par(mfrow=c(2,2))
plot(lin)
par(mfrow=c(1,1))

grp <- ggplot(data,aes(x=income,y=happiness))+ geom_point()
grp

grp <- grp + geom_smooth(method = "lm",col='blue')
grp

grp <- grp + stat_regline_equation(label.x = 4, label.y=8)
grp

d <- data.frame(income = c(2.314489,3.433490))

predictions = predict(lin,newdata = data)
plot(data$happiness,predictions)
abline(0,1)

print(str(predictions))

mse = mean((data$happiness - predictions)^2)
mse

rmse = sqrt(mean((data$income - predictions)^2))
rmse

print(median(data$happiness - predictions))

print(data$happiness[1:10])
print(predictions[1:3])

-------------

dataset = read.csv('Salary_Data.csv')

install.packages('caTools')
library(caTools)
library(ggplot2)


split = sample.split(dataset$Salary, SplitRatio = 0.7)
trainingset = subset(dataset, split == TRUE)
testset = subset(dataset, split == FALSE)

lm.r= lm(formula = Salary ~ YearsExperience, data = trainingset)
coef(lm.r)

summary(lm.r)

ypred = predict(lm.r, newdata = testset)

# Visualising the Training set results
ggplot() + geom_point(aes(x = trainingset$YearsExperience, y = trainingset$Salary), colour = 'red') +
geom_line(aes(x = trainingset$YearsExperience, y = predict(lm.r, newdata = trainingset)), colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') + xlab('Years of experience') + ylab('Salary')

# Visualising the Test set results
ggplot() +
  geom_point(aes(x = testset$YearsExperience, y = testset$Salary),
             colour = 'red') +
  geom_line(aes(x = trainingset$YearsExperience,
                y = predict(lm.r, newdata = trainingset)),
            colour = 'blue') +
  ggtitle('Salary vs Experience (Test set)') +
  xlab('Years of experience') +
  ylab('Salary')

------------------

lg reg:

install.packages("dplyr")
install.packages("ROCR")	 # For ROC curve to evaluate model

install.packages("ggcorrplot")
library(ggcorrplot)
library(caTools)
library(ROCR)
library(dplyr)

df = data(mtcars)
corr_mat <- cor(mtcars)
corrplot(corr_mat,method = "square")
ggcorrplot(corr_mat)
print(corr_mat)
head(mtcars)

split <- sample.split(mtcars, SplitRatio = 0.8)
split

train_reg <- subset(mtcars, split == "TRUE")
test_reg <- subset(mtcars, split == "FALSE")

logistic_model <- glm(vs ~ wt + disp,data = train_reg, family = "binomial")
logistic_model

summary(logistic_model)

predict_reg <- predict(logistic_model, test_reg, type = "response")
print(predict_reg)

predict_reg <- ifelse(predict_reg >0.5, 1, 0)

table(test_reg$vs, predict_reg)

missing_classerr <- mean(predict_reg != test_reg$vs)
print(paste('Accuracy =', 1 - missing_classerr))

# ROC-AUC Curve
ROCPred <- prediction(predict_reg, test_reg$vs)
ROCPer <- performance(ROCPred, measure = "tpr",
                      x.measure = "fpr")

auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc

# Plotting curve
plot(ROCPer)
plot(ROCPer, colorize = TRUE,
     print.cutoffs.at = seq(0.1, by = 0.1),
     main = "ROC CURVE")
abline(a = 0, b = 1)

auc <- round(auc, 4)
legend(.6, .4, auc, title = "AUC", cex = 1)

-----------

library("caTools")
library("tidyverse")
install.packages("tidyverse")
titanic_df <- read.csv("Titanic.csv")

summary(titanic_df)

data <- select(titanic_df, c(2, 3, 5, 6, 12))
head(data)

data$Age[is.na(data$Age)] <- mean(data$Age, na.rm = T)
data$Sex <- as.numeric(factor(data$Sex))
data$Embarked <- as.numeric(factor(data$Embarked))

print(data)

split <- sample.split(data, SplitRatio = 0.7)
train <- subset(data, split == "TRUE")
test <- subset(data, split == "FALSE")

print(train)
print(test)

model <- glm(Survived ~ . , family = binomial(link = "logit"), data = train)
summary(model)

y_pred <- predict(model, select(test, c(2, 3, 4, 5)))
print(y_pred)

#Now, print AUC and ROC curves

library("ROCR")

pr <- prediction(y_pred, test$Survived)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)


---------------
RandomForest

data(iris)

str(iris)

install.packages("caTools")
install.packages("randomForest")

library("caTools")
library("randomForest")

split <- sample.split(iris, SplitRatio = 0.7)

print(split)

train <- subset(iris, split == "TRUE")
test <- subset(iris, split == "FALSE")

print(train)
print(test)

set.seed(120)


classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)

print(classifier_RF)

pred_result <- predict(classifier_RF, test[-5])
print(pred_result)

print(test[, 5])

confusion_mtx = table(test[, 5], pred_result)
print(confusion_mtx)

plot(classifier_RF)

importance(classifier_RF)

varImpPlot(classifier_RF)

acc=sum(diag(confusion_mtx)/sum(confusion_mtx))
acc

-------------------

kmeans:

install.packages("dplyr")
install.packages("ggplot2")
install.packages("ggfortify")

library("dplyr")
library("ggplot2")
library("ggfortify")

data(iris)
head(iris)

data <- select(iris, c(1 : 4))

wssplot <- function(data, nc = 15, seed = 1234) {
  
  wss <- (nrow(data) - 1) * sum(apply(data, 2, var))
  
  for(i in 2 : nc) {
    set.seed(1234)
    
    wss[i] <- sum(kmeans(data, i)$withinss)
  }
  
  plot(1 : nc, wss, type = "b", xlab = "Number of clusters", ylab = "Within groups sum of squares")
  print(wss)
}

wssplot(data)

classifier_RF <- kmeans(data, 2)
print(classifier_RF)
print(classifier_RF$centers)

autoplot(classifier_RF, data, frame = "TRUE")

--------------------
# Installing Packages
install.packages("ClusterR")
install.packages("cluster")

# Loading package
library(ClusterR)
library(cluster)

# Removing initial label of
# Species from original dataset
iris_1 <- iris[, -5]

# Fitting K-Means clustering Model
# to training dataset
set.seed(240) # Setting seed
kmeans.re <- kmeans(iris_1, centers = 3, nstart = 20)
kmeans.re

# Cluster identification for
# each observation
kmeans.re$cluster

# Confusion Matrix
cm <- table(iris$Species, kmeans.re$cluster)
cm

# Model Evaluation and visualization
plot(iris_1[c("Sepal.Length", "Sepal.Width")])
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
     col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
     col = kmeans.re$cluster,
     main = "K-means with 3 clusters")

## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]

# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
       col = 1:3, pch = 8, cex = 3)

## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster iris"),
         xlab = 'Sepal.Length',
         ylab = 'Sepal.Width')

---------------------

hclust:

install.packages("dplyr")
library(dplyr)
head(mtcars)
distance_mat <- dist(mtcars, method = 'euclidean')
distance_mat
set.seed(240)  # Setting seed
Hierar_cl <- hclust(distance_mat, method = "average")
Hierar_cl
plot(Hierar_cl)
abline(h = 110, col = "green")
fit <- cutree(Hierar_cl, k = 3 )
fit
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")

---------------
install.packages("factoextra")

library("dplyr")
library("factoextra")
head(mtcars)

dist_mtx <- dist(mtcars, method = "euclidean")
print(dist_mtx)

set.seed(240)
model <- hclust(dist_mtx, method = "average")
plot(model)

sub_grps <- cutree(model, k = 3)

fviz_cluster(list(data = mtcars, cluster = sub_grps))
------------

Association:


install.packages("arules")
install.packages("arulesViz")

library(arules)
library(arulesViz)


dataset = read.transactions('Market_Basket_Optimisation.csv', sep=',', rm.duplicates = TRUE)
str(dataset)

set.seed = 220 
associa_rules = apriori(data = dataset,parameter = list(support = 0.004, confidence = 0.2))

# Plot
itemFrequencyPlot(dataset, topN = 10)

# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10])
plot(associa_rules, method = "graph", measure = "confidence", shading = "lift")
